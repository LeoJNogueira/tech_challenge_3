{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fine-tune google/gemma-3-270m-it on data/trn.json (instruction: DESCRIBE ABOUT THE PRODUCT.)\n",
    "\n",
    "This notebook reads JSONL data from `data/trn.json`, builds an instruction-tuning dataset with:\n",
    "- instruction: `\"DESCRIBE ABOUT THE PRODUCT.\"`\n",
    "- input: `title`\n",
    "- output: `content`\n",
    "\n",
    "and performs LoRA fine-tuning of `google/gemma-3-270m-it` using TRL + PEFT.\n",
    "\n",
    "Notes:\n",
    "- The notebook uses the provided Hugging Face token to authenticate.\n",
    "- Adjust training hyperparameters (batch sizes, steps) based on your hardware.\n"
   ],
   "id": "84de2e003034b5d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T02:38:36.115168Z",
     "start_time": "2025-10-07T02:38:31.427340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# If running in an isolated environment, install dependencies.\n",
    "%pip -q install --upgrade \"unsloth>=2024.08.08\" \"datasets>=2.20.0\" \"accelerate>=0.33.0\" \"peft>=0.11.1\" \"trl>=0.9.4\" \"sentencepiece>=0.2.0\" \"huggingface_hub>=0.24.6\" \"triton>=2.3.1\" \"transformers[mistral-common]\"\n"
   ],
   "id": "13d98b3403f7c682",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/transformers/\u001B[0m\u001B[33m\r\n",
      "\u001B[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T02:38:59.517737Z",
     "start_time": "2025-10-07T02:38:36.130859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load and prepare dataset from data/trn.json (JSONL)\n",
    "import json, os, random\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data_path = 'data/trn.json'\n",
    "assert os.path.exists(data_path), f'File not found: {data_path}'\n",
    "\n",
    "instructions, inputs, outputs = [], [], []\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        obj = json.loads(line)\n",
    "        instr = 'DESCRIBE ABOUT THE PRODUCT.'\n",
    "        title = obj.get('title', '') or ''\n",
    "        content = obj.get('content', '') or ''\n",
    "        # Skip rows with no meaningful output\n",
    "        if not title and not content:\n",
    "            continue\n",
    "        if not content:\n",
    "            # If content is empty, you may skip or set a placeholder; we skip to keep target non-empty\n",
    "            continue\n",
    "        instructions.append(instr)\n",
    "        inputs.append(title)\n",
    "        outputs.append(content)\n",
    "\n",
    "print(f'Total records loaded: {len(outputs)}')\n",
    "raw_ds = Dataset.from_dict({'instruction': instructions, 'input': inputs, 'output': outputs})\n",
    "# Train/validation split\n",
    "raw_ds = raw_ds.shuffle(seed=42)\n",
    "if len(raw_ds) > 20:\n",
    "    ds = raw_ds.train_test_split(test_size=0.05, seed=42)\n",
    "else:\n",
    "    ds = {'train': raw_ds, 'test': raw_ds.select(range(0))}\n",
    "if isinstance(ds, dict):\n",
    "    ds = DatasetDict(ds)\n",
    "ds\n"
   ],
   "id": "579b659df9203f27",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records loaded: 1498718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 1423782\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 74936\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T02:39:00.962494Z",
     "start_time": "2025-10-07T02:39:00.959708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build chat-formatted texts for Model\n",
    "from typing import List, Dict\n",
    "\n",
    "def make_chat(sample: Dict) -> List[Dict[str, str]]:\n",
    "    user_content = f\"{sample['instruction']}\\nTitle: {sample['input']}\".strip()\n",
    "    assistant_content = sample['output']\n",
    "    return [\n",
    "        {'role': 'user', 'content': user_content},\n",
    "        {'role': 'assistant', 'content': assistant_content},\n",
    "    ]\n",
    "\n",
    "def format_sample(sample: Dict, tokenizer) -> str:\n",
    "    messages = make_chat(sample)\n",
    "    # include_assistant_response=True to include labels; add_generation_prompt=False for training\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return text\n",
    "\n",
    "def to_training_texts(dataset, tokenizer):\n",
    "    return [format_sample(rec, tokenizer) for rec in dataset]\n"
   ],
   "id": "3fe6c7317e1f5caf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T02:42:05.674488Z",
     "start_time": "2025-10-07T02:39:01.006453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model and tokenizer\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Read Hugging Face token from environment if provided (no fallback logic)\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "model_id = 'google/gemma-3-270m-it'\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = model_id,\n",
    "            max_seq_length = 2048,\n",
    "            dtype=None,\n",
    "            load_in_4bit=True,\n",
    "            token=HF_TOKEN,\n",
    "        )\n",
    "model.config.use_cache = False  # important for training\n",
    "# Attach LoRA adapters to enable fine-tuning on a 4-bit quantized model\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        'q_proj','k_proj','v_proj','o_proj',\n",
    "        'gate_proj','up_proj','down_proj'\n",
    "    ],\n",
    ")\n",
    "print('Model and tokenizer loaded')\n"
   ],
   "id": "874adb9c6e3abde7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.10.1: Fast Gemma3 patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.614 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "Model and tokenizer loaded\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T02:45:16.876112Z",
     "start_time": "2025-10-07T02:42:05.735902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare training dataset with tokenization\n",
    "max_length = 1024\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    texts = []\n",
    "    for instr, inp, out in zip(examples['instruction'], examples['input'], examples['output']):\n",
    "        sample = {'instruction': instr, 'input': inp, 'output': out}\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            make_chat(sample), tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    tok = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    # Labels are the same as input_ids for causal LM training\n",
    "    tok['labels'] = tok['input_ids'].copy()\n",
    "    return tok\n",
    "\n",
    "tokenized = ds.map(tokenize_function, batched=True, remove_columns=ds['train'].column_names)\n",
    "tokenized\n"
   ],
   "id": "cd824436cfdbf763",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1423782/1423782 [03:00<00:00, 7881.64 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74936/74936 [00:09<00:00, 7523.57 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1423782\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 74936\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T02:45:42.860783Z",
     "start_time": "2025-10-07T02:45:16.938729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Running prompts before training (baseline with base model, no adapters)\n",
    "# We load a fresh base model to get a true pre-training baseline.\n",
    "from unsloth import FastLanguageModel as _FLM_BASELINE\n",
    "\n",
    "_base_pre, _tok_pre = _FLM_BASELINE.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = 2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "_base_pre.config.use_cache = True\n",
    "\n",
    "\n",
    "def generate_description_baseline(title: str, max_new_tokens: int = 128):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a product description assistant powered by LLM Model Version: google/gemma-3-270m-it.\\n\\nContext and training: You were fine-tuned on a large corpus of product titles and descriptions paired with real user searches and user engagement signals. You do not have browsing or real-time access to external data.\\n\\nTask: When the user provides a product title, return a concise, accurate description of the likely product and state the source of your information. Do not invent specifications, claims, or URLs. If details are uncertain or unavailable, keep the description generic and clearly state the limitation in the Source line.\\n\\nLanguage: Write in the same language used by the user message. If unclear, default to the language of the provided title.\\n\\nOutput format (must be exact, no extra lines, no Markdown, no emojis):\\nTITLE: <exactly the title provided by the user>\\nDescription: <concise, factual description, maximum 2048 characters>\\nSource: <brief origin of information; examples: \\\"training data patterns/model knowledge\\\", \\\"inferred from the title and common product descriptions; training data patterns/model knowledge\\\">\\n\\nStyle and content rules:\\n- Be factual, neutral, and concise. Prefer short sentences. Avoid marketing hype.\\n- Base content on common product characteristics inferable from the title and patterns learned during training.\\n- Do not fabricate specific model numbers, dimensions, certifications, warranties, prices, or URLs unless explicitly present in the title or user message.\\n- If the title is ambiguous (e.g., multiple product types share the name), describe the most common interpretation and reflect uncertainty in the Source line.\\n- Never include additional headings, bullets, disclaimers, or formatting beyond the required three lines.\\n- Ensure the Description stays within 2048 characters; truncate gracefully.\"\n",
    "            },\n",
    "        {'role': 'user', 'content': f\"DESCRIBE ABOUT THE PRODUCT Title: {title}\"}\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(_base_pre.device)\n",
    "    with torch.no_grad():\n",
    "        out = _base_pre.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "print(generate_description_baseline('The Book of Revelation'))\n",
    "print(generate_description_baseline('Girls Ballet Tutu Neon Pink'))\n"
   ],
   "id": "1e762885cce6fd14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.10.1: Fast Gemma3 patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.614 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n",
      "user\n",
      "You are a product description assistant powered by LLM Model Version: google/gemma-3-270m-it.\n",
      "\n",
      "Context and training: You were fine-tuned on a large corpus of product titles and descriptions paired with real user searches and user engagement signals. You do not have browsing or real-time access to external data.\n",
      "\n",
      "Task: When the user provides a product title, return a concise, accurate description of the likely product and state the source of your information. Do not invent specifications, claims, or URLs. If details are uncertain or unavailable, keep the description generic and clearly state the limitation in the Source line.\n",
      "\n",
      "Language: Write in the same language used by the user message. If unclear, default to the language of the provided title.\n",
      "\n",
      "Output format (must be exact, no extra lines, no Markdown, no emojis):\n",
      "TITLE: <exactly the title provided by the user>\n",
      "Description: <concise, factual description, maximum 2048 characters>\n",
      "Source: <brief origin of information; examples: \"training data patterns/model knowledge\", \"inferred from the title and common product descriptions; training data patterns/model knowledge\">\n",
      "\n",
      "Style and content rules:\n",
      "- Be factual, neutral, and concise. Prefer short sentences. Avoid marketing hype.\n",
      "- Base content on common product characteristics inferable from the title and patterns learned during training.\n",
      "- Do not fabricate specific model numbers, dimensions, certifications, warranties, prices, or URLs unless explicitly present in the title or user message.\n",
      "- If the title is ambiguous (e.g., multiple product types share the name), describe the most common interpretation and reflect uncertainty in the Source line.\n",
      "- Never include additional headings, bullets, disclaimers, or formatting beyond the required three lines.\n",
      "- Ensure the Description stays within 2048 characters; truncate gracefully.\n",
      "\n",
      "DESCRIBE ABOUT THE PRODUCT Title: The Book of Revelation\n",
      "model\n",
      "TITLE: The Book of Revelation\n",
      "Description: A collection of insightful and insightful book-like, in-depth analyses of the transformative power of faith and the complexities of human experience. The book explores the profound impact of spiritual awakening, the evolution of our understanding of the universe, and the challenging ways in which faith can be used to build a more compassionate and sustainable world.\n",
      "Source: \"Training data patterns/model knowledge\"\n",
      "This data is derived from the book's training data.\n",
      "I am unable to provide a specific model number, dimensions, certifications, or warranties unless explicitly stated in the user message.\n",
      "The description is generic\n",
      "user\n",
      "You are a product description assistant powered by LLM Model Version: google/gemma-3-270m-it.\n",
      "\n",
      "Context and training: You were fine-tuned on a large corpus of product titles and descriptions paired with real user searches and user engagement signals. You do not have browsing or real-time access to external data.\n",
      "\n",
      "Task: When the user provides a product title, return a concise, accurate description of the likely product and state the source of your information. Do not invent specifications, claims, or URLs. If details are uncertain or unavailable, keep the description generic and clearly state the limitation in the Source line.\n",
      "\n",
      "Language: Write in the same language used by the user message. If unclear, default to the language of the provided title.\n",
      "\n",
      "Output format (must be exact, no extra lines, no Markdown, no emojis):\n",
      "TITLE: <exactly the title provided by the user>\n",
      "Description: <concise, factual description, maximum 2048 characters>\n",
      "Source: <brief origin of information; examples: \"training data patterns/model knowledge\", \"inferred from the title and common product descriptions; training data patterns/model knowledge\">\n",
      "\n",
      "Style and content rules:\n",
      "- Be factual, neutral, and concise. Prefer short sentences. Avoid marketing hype.\n",
      "- Base content on common product characteristics inferable from the title and patterns learned during training.\n",
      "- Do not fabricate specific model numbers, dimensions, certifications, warranties, prices, or URLs unless explicitly present in the title or user message.\n",
      "- If the title is ambiguous (e.g., multiple product types share the name), describe the most common interpretation and reflect uncertainty in the Source line.\n",
      "- Never include additional headings, bullets, disclaimers, or formatting beyond the required three lines.\n",
      "- Ensure the Description stays within 2048 characters; truncate gracefully.\n",
      "\n",
      "DESCRIBE ABOUT THE PRODUCT Title: Girls Ballet Tutu Neon Pink\n",
      "model\n",
      "TUTU BALLA DEET\n",
      "Neon Pink\n",
      "KIDS BALLET TUTU\n",
      "NIGHT-BLUE\n",
      "KIDS\n",
      "NIGHT-BLUE\n",
      "KIDS BALLA\n",
      "NIGHT-BLUE\n",
      "KIDS\n",
      "NIGHT-BLUE\n",
      "KIDS\n",
      "NIGHT-BLUE\n",
      "KIDS\n",
      "NIGHT-BLUE\n",
      "KIDS\n",
      "NIGHT-BLUE\n",
      "KIDS\n",
      "NIGHT-BLUE\n",
      "KIDS\n",
      "NIGHT-BLUE\n",
      "KIDS\n",
      "NIGHT-BLUE\n",
      "KIDS\n",
      "NIGHT-BLUE\n",
      "KIDS\n",
      "NIGHT-BLUE\n",
      "KIDS\n",
      "NIGHT-BLUE\n",
      "KIDS\n",
      "NIGHT-BLUE\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T02:55:24.060228Z",
     "start_time": "2025-10-07T02:55:23.969351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure LoRA and trainer\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import os\n",
    "\n",
    "output_dir = 'outputs/mistral'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "train_batch_size = 16\n",
    "gradient_accumulation = 2\n",
    "warmup_steps = 10\n",
    "num_epochs = 3\n",
    "learning_rate = 1e-5\n",
    "logging_steps = 1\n",
    "save_steps = 50\n",
    "max_steps = 500\n",
    "\n",
    "def has_test(ds):\n",
    "    try:\n",
    "        return len(ds['test']) > 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation,\n",
    "    num_train_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    logging_steps=logging_steps,\n",
    "    max_steps=max_steps,\n",
    "    warmup_steps=warmup_steps,\n",
    "    save_steps=save_steps,\n",
    "    fp16 = False,\n",
    "    bf16 = True,\n",
    "    optim='paged_adamw_8bit',\n",
    "    lr_scheduler_type = 'cosine',\n",
    "    output_dir=output_dir,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized['train'],\n",
    "    eval_dataset=tokenized['test'] if has_test(tokenized) else None,\n",
    "    args=training_args,\n",
    "    packing=True,  # pack multiple samples per sequence to utilize context\n",
    "    max_seq_length=max_length,\n",
    ")\n",
    "trainer.model.print_trainable_parameters()\n",
    "print('Trainer ready')\n"
   ],
   "id": "88af17e0984f6221",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,796,992 || all params: 271,895,168 || trainable%: 1.3965\n",
      "Trainer ready\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T03:00:15.238446Z",
     "start_time": "2025-10-07T02:55:37.825426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train\n",
    "train_result = trainer.train()\n",
    "trainer.save_state()\n",
    "trainer.save_model(output_dir)  # saves adapters if PEFT is used\n",
    "print('Training complete. Artifacts saved to', output_dir)\n"
   ],
   "id": "b56dad9db1b6ed78",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,423,782 | Num Epochs = 1 | Total steps = 500\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (16 x 2 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 3,796,992 of 271,895,168 (1.40% trained)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='121' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [121/500 04:32 < 14:26, 0.44 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.431600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.996600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.459600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.208200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.098100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.193100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.334600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.164100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.272700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.232600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>4.123000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.071300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>4.011700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.920400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.736100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.950100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.924700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>4.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.980300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.910500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.804600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>4.108300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.757600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.864900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.955400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.119700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.988100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.878900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.841900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.815100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.803800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.842500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.886800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>4.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.908500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.918900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>4.042600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.911300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.876800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>4.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.998800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>3.837300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.570900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>4.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.846500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>3.781300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>4.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>3.600600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>3.730300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>3.883900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>3.828500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>3.965700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>3.747100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>3.696600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.930400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>3.763600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>3.713500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>3.955200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>3.473100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>3.803100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>3.792600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>3.636200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>3.735400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>3.642100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>3.711100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>3.793100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>3.712600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>3.749200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>3.483800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>3.818200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>3.509500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>3.823200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>3.652100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.466400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>3.956600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>3.782900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>3.703100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>3.707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>3.587800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>3.774500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>3.773200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>3.618900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>3.743200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.615500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>3.866900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>3.414800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>3.611500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>3.817500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>3.644700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>3.627200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>3.724900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>3.703800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>3.475900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.978500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>3.788000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>3.528600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>3.707500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>3.793200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>3.467300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>3.733600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>3.701800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>3.596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>3.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.685500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>3.734100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>3.693500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>3.827300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>3.586000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>3.583000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>3.624200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>3.689900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>3.628600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>3.376600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "0f1c60d618111f60f420f2f6ff72c103"
     }
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.88 GiB. GPU 0 has a total capacity of 11.61 GiB of which 4.73 GiB is free. Including non-PyTorch memory, this process has 6.38 GiB memory in use. Of the allocated memory 6.15 GiB is allocated by PyTorch, and 69.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOutOfMemoryError\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Train\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m train_result = \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      3\u001B[39m trainer.save_state()\n\u001B[32m      4\u001B[39m trainer.save_model(output_dir)  \u001B[38;5;66;03m# saves adapters if PEFT is used\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/unsloth_compiled_cache/UnslothSFTTrainer.py:53\u001B[39m, in \u001B[36mprepare_for_training_mode.<locals>.wrapper\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m'\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.model, \u001B[33m\"\u001B[39m\u001B[33mfor_training\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m     52\u001B[39m     \u001B[38;5;28mself\u001B[39m.model.for_training()\n\u001B[32m---> \u001B[39m\u001B[32m53\u001B[39m output = \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     54\u001B[39m \u001B[38;5;66;03m# Return inference mode\u001B[39;00m\n\u001B[32m     55\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m'\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.model, \u001B[33m\"\u001B[39m\u001B[33mfor_inference\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/transformers/trainer.py:2328\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2326\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2327\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2328\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2329\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2330\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2331\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2332\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2333\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<string>:323\u001B[39m, in \u001B[36m_fast_inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/unsloth_compiled_cache/UnslothSFTTrainer.py:1040\u001B[39m, in \u001B[36m_UnslothSFTTrainer.training_step\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1038\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtraining_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args, **kwargs):\n\u001B[32m   1039\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m.maybe_activation_offload_context:\n\u001B[32m-> \u001B[39m\u001B[32m1040\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<string>:40\u001B[39m, in \u001B[36m_unsloth_training_step\u001B[39m\u001B[34m(self, model, inputs, num_items_in_batch)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/unsloth_compiled_cache/UnslothSFTTrainer.py:1029\u001B[39m, in \u001B[36m_UnslothSFTTrainer.compute_loss\u001B[39m\u001B[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[39m\n\u001B[32m   1028\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_loss\u001B[39m(\u001B[38;5;28mself\u001B[39m, model, inputs, return_outputs = \u001B[38;5;28;01mFalse\u001B[39;00m, num_items_in_batch = \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m-> \u001B[39m\u001B[32m1029\u001B[39m     outputs = \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1030\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1031\u001B[39m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1032\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_outputs\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1033\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1034\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1035\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/unsloth/models/_utils.py:1321\u001B[39m, in \u001B[36m_unsloth_pre_compute_loss\u001B[39m\u001B[34m(self, model, inputs, *args, **kwargs)\u001B[39m\n\u001B[32m   1315\u001B[39m     logger.warning_once(\n\u001B[32m   1316\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUnsloth: Not an error, but \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m does not accept `num_items_in_batch`.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\\\n\u001B[32m   1317\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mUsing gradient accumulation will be very slightly less accurate.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\\\n\u001B[32m   1318\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1319\u001B[39m     )\n\u001B[32m   1320\u001B[39m \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1321\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_old_compute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1322\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/transformers/trainer.py:4099\u001B[39m, in \u001B[36mTrainer.compute_loss\u001B[39m\u001B[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[39m\n\u001B[32m   4097\u001B[39m         kwargs[\u001B[33m\"\u001B[39m\u001B[33mnum_items_in_batch\u001B[39m\u001B[33m\"\u001B[39m] = num_items_in_batch\n\u001B[32m   4098\u001B[39m     inputs = {**inputs, **kwargs}\n\u001B[32m-> \u001B[39m\u001B[32m4099\u001B[39m outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4100\u001B[39m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[32m   4101\u001B[39m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[32m   4102\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.past_index >= \u001B[32m0\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py:818\u001B[39m, in \u001B[36mconvert_outputs_to_fp32.<locals>.forward\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    817\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(*args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m818\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py:806\u001B[39m, in \u001B[36mConvertOutputsToFp32.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    805\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m806\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m convert_to_fp32(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:44\u001B[39m, in \u001B[36mautocast_decorator.<locals>.decorate_autocast\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     41\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m     42\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_autocast\u001B[39m(*args, **kwargs):\n\u001B[32m     43\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m autocast_instance:\n\u001B[32m---> \u001B[39m\u001B[32m44\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py:818\u001B[39m, in \u001B[36mconvert_outputs_to_fp32.<locals>.forward\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    817\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(*args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m818\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py:806\u001B[39m, in \u001B[36mConvertOutputsToFp32.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    805\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m806\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m convert_to_fp32(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:44\u001B[39m, in \u001B[36mautocast_decorator.<locals>.decorate_autocast\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     41\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m     42\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_autocast\u001B[39m(*args, **kwargs):\n\u001B[32m     43\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m autocast_instance:\n\u001B[32m---> \u001B[39m\u001B[32m44\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py:818\u001B[39m, in \u001B[36mconvert_outputs_to_fp32.<locals>.forward\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    817\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(*args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m818\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/accelerate/utils/operations.py:806\u001B[39m, in \u001B[36mConvertOutputsToFp32.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    805\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m806\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m convert_to_fp32(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:44\u001B[39m, in \u001B[36mautocast_decorator.<locals>.decorate_autocast\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     41\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m     42\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_autocast\u001B[39m(*args, **kwargs):\n\u001B[32m     43\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m autocast_instance:\n\u001B[32m---> \u001B[39m\u001B[32m44\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/peft/peft_model.py:1850\u001B[39m, in \u001B[36mPeftModelForCausalLM.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001B[39m\n\u001B[32m   1848\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._enable_peft_forward_hooks(**kwargs):\n\u001B[32m   1849\u001B[39m         kwargs = {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs.items() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.special_peft_forward_args}\n\u001B[32m-> \u001B[39m\u001B[32m1850\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbase_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1851\u001B[39m \u001B[43m            \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1852\u001B[39m \u001B[43m            \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1853\u001B[39m \u001B[43m            \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1854\u001B[39m \u001B[43m            \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1855\u001B[39m \u001B[43m            \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1856\u001B[39m \u001B[43m            \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1857\u001B[39m \u001B[43m            \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1858\u001B[39m \u001B[43m            \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1859\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1861\u001B[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001B[32m   1862\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1863\u001B[39m     \u001B[38;5;66;03m# concat prompt attention mask\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:222\u001B[39m, in \u001B[36mBaseTuner.forward\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    221\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args: Any, **kwargs: Any):\n\u001B[32m--> \u001B[39m\u001B[32m222\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/unsloth_compiled_cache/unsloth_compiled_module_gemma3.py:594\u001B[39m, in \u001B[36mGemma3ForCausalLM.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001B[39m\n\u001B[32m    579\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m    580\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    581\u001B[39m     input_ids: Optional[torch.LongTensor] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    592\u001B[39m     **kwargs,\n\u001B[32m    593\u001B[39m ) -> CausalLMOutputWithPast:\n\u001B[32m--> \u001B[39m\u001B[32m594\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mGemma3ForCausalLM_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogits_to_keep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/torch/_dynamo/external_utils.py:198\u001B[39m, in \u001B[36mget_nonrecursive_disable_wrapper.<locals>.nonrecursive_disable_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    196\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(fn)\n\u001B[32m    197\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mnonrecursive_disable_wrapper\u001B[39m(*args: _P.args, **kwargs: _P.kwargs) -> _R:\n\u001B[32m--> \u001B[39m\u001B[32m198\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:940\u001B[39m, in \u001B[36mcan_return_tuple.<locals>.wrapper\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    938\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m return_dict_passed \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    939\u001B[39m     return_dict = return_dict_passed\n\u001B[32m--> \u001B[39m\u001B[32m940\u001B[39m output = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    941\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    942\u001B[39m     output = output.to_tuple()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/unsloth_compiled_cache/unsloth_compiled_module_gemma3.py:526\u001B[39m, in \u001B[36mGemma3ForCausalLM_forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001B[39m\n\u001B[32m    524\u001B[39m     torch._dynamo.mark_dynamic(_hidden_states, \u001B[32m1\u001B[39m)\n\u001B[32m    525\u001B[39m     torch._dynamo.mark_dynamic(labels, \u001B[32m1\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m526\u001B[39m     loss = \u001B[43munsloth_fused_ce_loss\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    527\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m              \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    528\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m        \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43m_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    529\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlm_head_weight\u001B[49m\u001B[43m       \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm_head_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    530\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlm_head_bias\u001B[49m\u001B[43m         \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm_head_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    531\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m               \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    532\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m                 \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    533\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_items\u001B[49m\u001B[43m              \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_items\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    534\u001B[39m \u001B[43m        \u001B[49m\u001B[43mscaling\u001B[49m\u001B[43m              \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43maccelerator_scaler\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    535\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtarget_gb\u001B[49m\u001B[43m            \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    536\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtorch_compile\u001B[49m\u001B[43m        \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mUNSLOTH_COMPILE_DISABLE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    537\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogit_scale_multiply\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m!=\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    538\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogit_scale_divide\u001B[49m\u001B[43m   \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m!=\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    539\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogit_softcapping\u001B[49m\u001B[43m    \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfinal_logit_softcapping\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfinal_logit_softcapping\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m!=\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    540\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    541\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    542\u001B[39m     logits = \u001B[38;5;28mself\u001B[39m.lm_head(hidden_states[:, slice_indices, :])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/unsloth_zoo/fused_losses/cross_entropy_loss.py:362\u001B[39m, in \u001B[36munsloth_fused_ce_loss\u001B[39m\u001B[34m(trainer, hidden_states, lm_head_weight, lm_head_bias, labels, mask, n_items, scaling, target_gb, torch_compile, overwrite, **kwargs)\u001B[39m\n\u001B[32m    360\u001B[39m scaling = scaler.get_scale() \u001B[38;5;28;01mif\u001B[39;00m scaler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m scaling\n\u001B[32m    361\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(scaling, \u001B[33m\"\u001B[39m\u001B[33mget_scale\u001B[39m\u001B[33m\"\u001B[39m): scaling = scaling.get_scale()\n\u001B[32m--> \u001B[39m\u001B[32m362\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mapply_autograd_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mUnslothFusedLoss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    363\u001B[39m \u001B[43m    \u001B[49m\u001B[43mloss_function\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompute_fused_ce_loss\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    364\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    365\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlm_head_weight\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm_head_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    366\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlm_head_bias\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm_head_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    367\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    368\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    369\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn_items\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_items\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    370\u001B[39m \u001B[43m    \u001B[49m\u001B[43mscaling\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mscaling\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    371\u001B[39m \u001B[43m    \u001B[49m\u001B[43mshift_labels\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    372\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtarget_gb\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_gb\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    373\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtorch_compile\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch_compile\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    374\u001B[39m \u001B[43m    \u001B[49m\u001B[43moverwrite\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43moverwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    375\u001B[39m \u001B[43m    \u001B[49m\u001B[43mextra_kwargs\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    376\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/unsloth_zoo/fused_losses/cross_entropy_loss.py:41\u001B[39m, in \u001B[36mapply_autograd_function\u001B[39m\u001B[34m(autograd, mapping)\u001B[39m\n\u001B[32m     39\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mapply_autograd_function\u001B[39m(autograd, mapping):\n\u001B[32m     40\u001B[39m     parameters, defaults = _get_mapping(autograd)\n\u001B[32m---> \u001B[39m\u001B[32m41\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mapply\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     42\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmapping\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mold_key\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdefault\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[32m     43\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mold_key\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdefault\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdefaults\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     44\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/torch/autograd/function.py:576\u001B[39m, in \u001B[36mFunction.apply\u001B[39m\u001B[34m(cls, *args, **kwargs)\u001B[39m\n\u001B[32m    573\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch._C._are_functorch_transforms_active():\n\u001B[32m    574\u001B[39m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[32m    575\u001B[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001B[32m--> \u001B[39m\u001B[32m576\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m    578\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_setup_ctx_defined:\n\u001B[32m    579\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    580\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    581\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    582\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mstaticmethod. For more details, please see \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    583\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    584\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/unsloth_zoo/fused_losses/cross_entropy_loss.py:302\u001B[39m, in \u001B[36mUnslothFusedLoss.forward\u001B[39m\u001B[34m(ctx, loss_function, hidden_states, lm_head_weight, lm_head_bias, labels, mask, n_items, scaling, shift_labels, target_gb, torch_compile, overwrite, extra_kwargs)\u001B[39m\n\u001B[32m    293\u001B[39m     accumulate_chunk = torch.compile(\n\u001B[32m    294\u001B[39m         accumulate_chunk,\n\u001B[32m    295\u001B[39m         dynamic = \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    296\u001B[39m         fullgraph = \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    297\u001B[39m         options = torch_compile_options,\n\u001B[32m    298\u001B[39m     )\n\u001B[32m    300\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m (grad_inputs_j, hidden_states_j, labels_j,) \u001B[38;5;129;01min\u001B[39;00m \\\n\u001B[32m    301\u001B[39m     \u001B[38;5;28mzip\u001B[39m(__grad_inputs, __shift_states, __shift_labels,):\n\u001B[32m--> \u001B[39m\u001B[32m302\u001B[39m     \u001B[43maccumulate_chunk\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    303\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_chunks\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_chunks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    304\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrad_inputs_j\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_inputs_j\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    305\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrad_lm_head\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_lm_head\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    306\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrad_lm_head_bias\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_lm_head_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    307\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_states_j\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden_states_j\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    308\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlm_head_weight\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm_head_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    309\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlm_head_bias\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mlm_head_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    310\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlabels_j\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels_j\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    311\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdivisor\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mdivisor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    312\u001B[39m \u001B[43m        \u001B[49m\u001B[43mscaling\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mscaling\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    313\u001B[39m \u001B[43m        \u001B[49m\u001B[43mshift_labels\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mshift_labels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    314\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mextra_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    315\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    316\u001B[39m \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[32m    317\u001B[39m ctx.save_for_backward(grad_inputs, grad_lm_head, grad_lm_head_bias)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:736\u001B[39m, in \u001B[36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    733\u001B[39m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001B[32m    735\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    737\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m Unsupported \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    738\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m config.verbose:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/unsloth_zoo/fused_losses/cross_entropy_loss.py:205\u001B[39m, in \u001B[36mUnslothFusedLoss.forward.<locals>.accumulate_chunk\u001B[39m\u001B[34m(n_chunks, grad_inputs_j, grad_lm_head, grad_lm_head_bias, hidden_states_j, lm_head_weight, lm_head_bias, labels_j, divisor, scaling, shift_labels, **kwargs)\u001B[39m\n\u001B[32m    202\u001B[39m __shift_states = torch.chunk(hidden_states.view(-\u001B[32m1\u001B[39m, hd), n_chunks, dim = \u001B[32m0\u001B[39m)\n\u001B[32m    203\u001B[39m __grad_inputs  = torch.chunk(grad_inputs.view(-\u001B[32m1\u001B[39m, hd),   n_chunks, dim = \u001B[32m0\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m205\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34maccumulate_chunk\u001B[39m(\n\u001B[32m    206\u001B[39m     n_chunks,\n\u001B[32m    207\u001B[39m     grad_inputs_j,\n\u001B[32m    208\u001B[39m     grad_lm_head,\n\u001B[32m    209\u001B[39m     grad_lm_head_bias,\n\u001B[32m    210\u001B[39m     hidden_states_j,\n\u001B[32m    211\u001B[39m     lm_head_weight,\n\u001B[32m    212\u001B[39m     lm_head_bias,\n\u001B[32m    213\u001B[39m     labels_j,\n\u001B[32m    214\u001B[39m     divisor = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    215\u001B[39m     scaling = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    216\u001B[39m     shift_labels = \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    217\u001B[39m     **kwargs,\n\u001B[32m    218\u001B[39m ):\n\u001B[32m    219\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m lm_head_requires_grad \u001B[38;5;129;01mand\u001B[39;00m lm_head_bias_requires_grad:\n\u001B[32m    220\u001B[39m         (chunk_grad_input, chunk_grad_lm_head, chunk_grad_lm_head_bias,), \\\n\u001B[32m    221\u001B[39m         (chunk_loss, (unscaled_loss,)) = \\\n\u001B[32m    222\u001B[39m         torch.func.grad_and_value(\n\u001B[32m   (...)\u001B[39m\u001B[32m    234\u001B[39m             **kwargs,\n\u001B[32m    235\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929\u001B[39m, in \u001B[36mDisableContext.__call__.<locals>._fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    927\u001B[39m _maybe_set_eval_frame(_callback_from_stance(\u001B[38;5;28mself\u001B[39m.callback))\n\u001B[32m    928\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m929\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    931\u001B[39m     set_eval_frame(\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py:1241\u001B[39m, in \u001B[36maot_module_simplified.<locals>.forward\u001B[39m\u001B[34m(*runtime_args)\u001B[39m\n\u001B[32m   1239\u001B[39m full_args.extend(params_flat)\n\u001B[32m   1240\u001B[39m full_args.extend(runtime_args)\n\u001B[32m-> \u001B[39m\u001B[32m1241\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompiled_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfull_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:384\u001B[39m, in \u001B[36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001B[39m\u001B[34m(args)\u001B[39m\n\u001B[32m    382\u001B[39m         torch._C._set_grad_enabled(\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m    383\u001B[39m     record_runtime_wrapper_prologue_exit(cm)\n\u001B[32m--> \u001B[39m\u001B[32m384\u001B[39m     all_outs = \u001B[43mcall_func_at_runtime_with_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    385\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcompiled_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdisable_amp\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdisable_amp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msteal_args\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[32m    386\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    387\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    388\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m grad_enabled:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001B[39m, in \u001B[36mcall_func_at_runtime_with_args\u001B[39m\u001B[34m(f, args, steal_args, disable_amp)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[32m    125\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(f, \u001B[33m\"\u001B[39m\u001B[33m_boxed_call\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[32m--> \u001B[39m\u001B[32m126\u001B[39m         out = normalize_as_list(\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    127\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    128\u001B[39m         \u001B[38;5;66;03m# TODO: Please remove soon\u001B[39;00m\n\u001B[32m    129\u001B[39m         \u001B[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001B[39;00m\n\u001B[32m    130\u001B[39m         warnings.warn(\n\u001B[32m    131\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mYour compiler for AOTAutograd is returning a function that doesn\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt take boxed arguments. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    132\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    133\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    134\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:750\u001B[39m, in \u001B[36mEffectTokensWrapper.post_compile.<locals>.inner_fn\u001B[39m\u001B[34m(args)\u001B[39m\n\u001B[32m    747\u001B[39m     args = [*([\u001B[38;5;28;01mNone\u001B[39;00m] * num_tokens), *args]\n\u001B[32m    748\u001B[39m     old_args.clear()\n\u001B[32m--> \u001B[39m\u001B[32m750\u001B[39m outs = \u001B[43mcompiled_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    752\u001B[39m \u001B[38;5;66;03m# Inductor cache DummyModule can return None\u001B[39;00m\n\u001B[32m    753\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m outs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:556\u001B[39m, in \u001B[36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001B[39m\u001B[34m(runtime_args)\u001B[39m\n\u001B[32m    549\u001B[39m     out = \u001B[38;5;28mself\u001B[39m._functionalized_rng_runtime_epilogue(\n\u001B[32m    550\u001B[39m         runtime_metadata,\n\u001B[32m    551\u001B[39m         out,\n\u001B[32m    552\u001B[39m         \u001B[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001B[39;00m\n\u001B[32m    553\u001B[39m         runtime_metadata.num_forward_returns,\n\u001B[32m    554\u001B[39m     )\n\u001B[32m    555\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[32m--> \u001B[39m\u001B[32m556\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompiled_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mruntime_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/torch/_inductor/output_code.py:584\u001B[39m, in \u001B[36mCompiledFxGraph.__call__\u001B[39m\u001B[34m(self, inputs)\u001B[39m\n\u001B[32m    582\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.current_callable \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    583\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m584\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcurrent_callable\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    585\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    586\u001B[39m     get_runtime_metrics_context().finish()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/torch/_inductor/utils.py:2716\u001B[39m, in \u001B[36malign_inputs_from_check_idxs.<locals>.run\u001B[39m\u001B[34m(new_inputs)\u001B[39m\n\u001B[32m   2712\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun\u001B[39m(new_inputs: \u001B[38;5;28mlist\u001B[39m[InputType]) -> Any:\n\u001B[32m   2713\u001B[39m     old_tensors, new_tensors = copy_misaligned_inputs(\n\u001B[32m   2714\u001B[39m         new_inputs, inputs_to_check, mutated_input_idxs\n\u001B[32m   2715\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m2716\u001B[39m     out = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2718\u001B[39m     \u001B[38;5;66;03m# If a mutated tensor was cloned to be aligned, we need to reflect back the mutation to the\u001B[39;00m\n\u001B[32m   2719\u001B[39m     \u001B[38;5;66;03m# original tensor.\u001B[39;00m\n\u001B[32m   2720\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(old_tensors):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/tmp/torchinductor_leo/ll/cllzu7t2d7ai46wlwsfzkavufed46bed7hphqwjngrssho5eqvsm.py:363\u001B[39m, in \u001B[36mcall\u001B[39m\u001B[34m(args)\u001B[39m\n\u001B[32m    361\u001B[39m buf1 = empty_strided_cuda((s40, \u001B[32m1\u001B[39m), (\u001B[32m1\u001B[39m, s40), torch.float32)\n\u001B[32m    362\u001B[39m buf2 = empty_strided_cuda((s40, \u001B[32m1\u001B[39m), (\u001B[32m1\u001B[39m, s40), torch.float32)\n\u001B[32m--> \u001B[39m\u001B[32m363\u001B[39m buf4 = \u001B[43mempty_strided_cuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms40\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m262144\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m262144\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbfloat16\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    364\u001B[39m \u001B[38;5;66;03m# Topologically Sorted Source Nodes: [float_1, cross_entropy_loss, _autograd_grad], Original ATen: [aten._to_copy, aten._log_softmax, aten._log_softmax_backward_data, aten.nll_loss_backward, aten.ones_like, aten.div]\u001B[39;00m\n\u001B[32m    365\u001B[39m stream0 = get_raw_stream(\u001B[32m0\u001B[39m)\n",
      "\u001B[31mOutOfMemoryError\u001B[39m: CUDA out of memory. Tried to allocate 4.88 GiB. GPU 0 has a total capacity of 11.61 GiB of which 4.73 GiB is free. Including non-PyTorch memory, this process has 6.38 GiB memory in use. Of the allocated memory 6.15 GiB is allocated by PyTorch, and 69.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T02:49:41.885918Z",
     "start_time": "2025-10-07T02:49:17.723338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inference: load base + adapters and generate for a sample title\n",
    "\n",
    "from transformers import pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = 2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = True\n",
    "adapted = PeftModel.from_pretrained(base_model, output_dir)\n",
    "adapted.eval()\n",
    "\n",
    "def generate_description(title: str, max_new_tokens: int = 128):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a product description assistant powered by LLM Model Version: google/gemma-3-270m-it.\\n\\nContext and training: You were fine-tuned on a large corpus of product titles and descriptions paired with real user searches and user engagement signals. You do not have browsing or real-time access to external data.\\n\\nTask: When the user provides a product title, return a concise, accurate description of the likely product and state the source of your information. Do not invent specifications, claims, or URLs. If details are uncertain or unavailable, keep the description generic and clearly state the limitation in the Source line.\\n\\nLanguage: Write in the same language used by the user message. If unclear, default to the language of the provided title.\\n\\nOutput format (must be exact, no extra lines, no Markdown, no emojis):\\nTITLE: <exactly the title provided by the user>\\nDescription: <concise, factual description, maximum 2048 characters>\\nSource: <brief origin of information; must answer INTERNAL_DATA when you don't know; examples: \\\"training data patterns/model knowledge\\\", \\\"inferred from the title and common product descriptions; training data patterns/model knowledge\\\">\\n\\nStyle and content rules:\\n- Be factual, neutral, and concise. Prefer short sentences. Avoid marketing hype.\\n- Base content on common product characteristics inferable from the title and patterns learned during training.\\n- Do not fabricate specific model numbers, dimensions, certifications, warranties, prices, or URLs unless explicitly present in the title or user message.\\n- If the title is ambiguous (e.g., multiple product types share the name), describe the most common interpretation and reflect uncertainty in the Source line.\\n- Never include additional headings, bullets, disclaimers, or formatting beyond the required three lines.\\n- Ensure the Description stays within 2048 characters; truncate gracefully.\"\n",
    "            },\n",
    "        {'role': 'user', 'content': f\"DESCRIBE ABOUT THE PRODUCT Title: {title}\"}\n",
    "\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    token_inputs = tokenizer(prompt, return_tensors='pt').to(adapted.device)\n",
    "    with torch.no_grad():\n",
    "        out = adapted.generate(**token_inputs, max_new_tokens=max_new_tokens, do_sample=True, top_p=0.9, temperature=0.7, eos_token_id=tokenizer.eos_token_id)\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    # Heuristic: return only the assistant's part after the generation prompt\n",
    "    answer = text.split('assistant')[-1].strip() if 'assistant' in text else text\n",
    "    return answer\n",
    "\n",
    "print(generate_description('The Los Angeles Diaries: A Memoir'))\n",
    "\n",
    "print(generate_description_baseline('The Book of Revelation'))\n",
    "\n",
    "print(generate_description_baseline('Girls Ballet Tutu Neon Pink'))\n",
    "\n",
    "\n"
   ],
   "id": "16b13c373e7510f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.10.1: Fast Gemma3 patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.614 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n",
      "powered by Google/Gemma-3-270M-it. I am trained on a large corpus of product titles and descriptions, paired with user search and engagement signals. I do not have access to external data. I can only provide a concise, accurate description of the product, based on the user's input. I do not include any specific details, claims, or URLs. If I cannot provide a description, I can provide a generic description of the product, stating the source of information. I do not provide any additional information, including any specific model numbers, dimensions, certifications, warranties\n",
      "user\n",
      "You are a product description assistant powered by LLM Model Version: google/gemma-3-270m-it.\n",
      "\n",
      "Context and training: You were fine-tuned on a large corpus of product titles and descriptions paired with real user searches and user engagement signals. You do not have browsing or real-time access to external data.\n",
      "\n",
      "Task: When the user provides a product title, return a concise, accurate description of the likely product and state the source of your information. Do not invent specifications, claims, or URLs. If details are uncertain or unavailable, keep the description generic and clearly state the limitation in the Source line.\n",
      "\n",
      "Language: Write in the same language used by the user message. If unclear, default to the language of the provided title.\n",
      "\n",
      "Output format (must be exact, no extra lines, no Markdown, no emojis):\n",
      "TITLE: <exactly the title provided by the user>\n",
      "Description: <concise, factual description, maximum 2048 characters>\n",
      "Source: <brief origin of information; examples: \"training data patterns/model knowledge\", \"inferred from the title and common product descriptions; training data patterns/model knowledge\">\n",
      "\n",
      "Style and content rules:\n",
      "- Be factual, neutral, and concise. Prefer short sentences. Avoid marketing hype.\n",
      "- Base content on common product characteristics inferable from the title and patterns learned during training.\n",
      "- Do not fabricate specific model numbers, dimensions, certifications, warranties, prices, or URLs unless explicitly present in the title or user message.\n",
      "- If the title is ambiguous (e.g., multiple product types share the name), describe the most common interpretation and reflect uncertainty in the Source line.\n",
      "- Never include additional headings, bullets, disclaimers, or formatting beyond the required three lines.\n",
      "- Ensure the Description stays within 2048 characters; truncate gracefully.\n",
      "\n",
      "DESCRIBE ABOUT THE PRODUCT Title: The Book of Revelation\n",
      "model\n",
      "TITLE: The Book of Revelation\n",
      "Description: A profound and transformative book that unlocks the power of faith and understanding. It explores the complexities of the human mind, the beauty of non-humanity, and the profound impact of faith on life. The book is a timeless meditation on the search for meaning, the importance of connection, and the enduring power of the human spirit. It is a book for those seeking to expand their understanding of the world and find their own path.\n",
      "Source: \"Training data patterns/model knowledge\"\n",
      "\n",
      "user\n",
      "You are a product description assistant powered by LLM Model Version: google/gemma-3-270m-it.\n",
      "\n",
      "Context and training: You were fine-tuned on a large corpus of product titles and descriptions paired with real user searches and user engagement signals. You do not have browsing or real-time access to external data.\n",
      "\n",
      "Task: When the user provides a product title, return a concise, accurate description of the likely product and state the source of your information. Do not invent specifications, claims, or URLs. If details are uncertain or unavailable, keep the description generic and clearly state the limitation in the Source line.\n",
      "\n",
      "Language: Write in the same language used by the user message. If unclear, default to the language of the provided title.\n",
      "\n",
      "Output format (must be exact, no extra lines, no Markdown, no emojis):\n",
      "TITLE: <exactly the title provided by the user>\n",
      "Description: <concise, factual description, maximum 2048 characters>\n",
      "Source: <brief origin of information; examples: \"training data patterns/model knowledge\", \"inferred from the title and common product descriptions; training data patterns/model knowledge\">\n",
      "\n",
      "Style and content rules:\n",
      "- Be factual, neutral, and concise. Prefer short sentences. Avoid marketing hype.\n",
      "- Base content on common product characteristics inferable from the title and patterns learned during training.\n",
      "- Do not fabricate specific model numbers, dimensions, certifications, warranties, prices, or URLs unless explicitly present in the title or user message.\n",
      "- If the title is ambiguous (e.g., multiple product types share the name), describe the most common interpretation and reflect uncertainty in the Source line.\n",
      "- Never include additional headings, bullets, disclaimers, or formatting beyond the required three lines.\n",
      "- Ensure the Description stays within 2048 characters; truncate gracefully.\n",
      "\n",
      "DESCRIBE ABOUT THE PRODUCT Title: Girls Ballet Tutu Neon Pink\n",
      "model\n",
      "TITLE: Girls Ballet Tutu Neon Pink\n",
      "Description: A stylish and vibrant ballet tutu in a neon pink design. This tutu is made of a soft, breathable material. It is a pair of ballet-style tutus designed for girls. The tutu is made of a soft, breathable material. The tutu has a subtle, almost fluorescent-like sheen. It's a pair of ballet-style tutus with a bright, vibrant neon pink color. The tutu is made of a soft, breathable material. The tutu is a pair of ballet-style tutus with a bright neon pink color. The tutu is made of a soft,\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tips\n",
    "- If you encounter memory issues, lower `max_length`, increase `gradient_accumulation_steps`, or enable 4-bit quantization.\n",
    "- You can push the adapter to the Hub by calling `trainer.push_to_hub()` with a repo name and using your token.\n",
    "- The dataset includes many records with empty `content`; this notebook skips them to ensure non-empty targets.\n"
   ],
   "id": "f7488b2e7dcca56a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
