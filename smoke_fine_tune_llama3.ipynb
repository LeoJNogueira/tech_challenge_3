{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fine-tune unsloth/Llama-3.2-1B-Instruct on data/trn.json (instruction: DESCRIBE ABOUT THE PRODUCT.)\n",
    "\n",
    "This notebook reads JSONL data from `data/trn.json`, builds an instruction-tuning dataset with:\n",
    "- instruction: `\"DESCRIBE ABOUT THE PRODUCT.\"`\n",
    "- input: `title`\n",
    "- output: `content`\n",
    "\n",
    "and performs LoRA fine-tuning of `unsloth/Llama-3.2-1B-Instruct` using TRL + PEFT.\n",
    "\n",
    "Notes:\n",
    "- The notebook uses the provided Hugging Face token to authenticate.\n",
    "- Adjust training hyperparameters (batch sizes, steps) based on your hardware.\n"
   ],
   "id": "84de2e003034b5d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T00:41:35.334811Z",
     "start_time": "2025-10-07T00:41:33.837113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# If running in an isolated environment, install dependencies.\n",
    "%pip -q install --upgrade \"unsloth>=2024.08.08\" \"transformers>=4.43.3\" \"datasets>=2.20.0\" \"accelerate>=0.33.0\" \"peft>=0.11.1\" \"trl>=0.9.4\" \"sentencepiece>=0.2.0\" \"huggingface_hub>=0.24.6\" \"triton>=2.3.1\"\n"
   ],
   "id": "13d98b3403f7c682",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T00:41:58.319232Z",
     "start_time": "2025-10-07T00:41:35.340116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load and prepare dataset from data/trn.json (JSONL)\n",
    "import json, os, random\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data_path = 'data/trn.json'\n",
    "assert os.path.exists(data_path), f'File not found: {data_path}'\n",
    "\n",
    "instructions, inputs, outputs = [], [], []\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        obj = json.loads(line)\n",
    "        instr = 'DESCRIBE ABOUT THE PRODUCT.'\n",
    "        title = obj.get('title', '') or ''\n",
    "        content = obj.get('content', '') or ''\n",
    "        # Skip rows with no meaningful output\n",
    "        if not title and not content:\n",
    "            continue\n",
    "        if not content:\n",
    "            # If content is empty, you may skip or set a placeholder; we skip to keep target non-empty\n",
    "            continue\n",
    "        instructions.append(instr)\n",
    "        inputs.append(title)\n",
    "        outputs.append(content)\n",
    "\n",
    "print(f'Total records loaded: {len(outputs)}')\n",
    "raw_ds = Dataset.from_dict({'instruction': instructions, 'input': inputs, 'output': outputs})\n",
    "# Train/validation split\n",
    "raw_ds = raw_ds.shuffle(seed=42)\n",
    "if len(raw_ds) > 20:\n",
    "    ds = raw_ds.train_test_split(test_size=0.05, seed=42)\n",
    "else:\n",
    "    ds = {'train': raw_ds, 'test': raw_ds.select(range(0))}\n",
    "if isinstance(ds, dict):\n",
    "    ds = DatasetDict(ds)\n",
    "ds\n"
   ],
   "id": "579b659df9203f27",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/PycharmProjects/TechClallenge_3/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records loaded: 1498718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 1423782\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 74936\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T00:41:59.608Z",
     "start_time": "2025-10-07T00:41:59.603457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build chat-formatted texts for Model\n",
    "from typing import List, Dict\n",
    "\n",
    "def make_chat(sample: Dict) -> List[Dict[str, str]]:\n",
    "    user_content = f\"{sample['instruction']}\\nTitle: {sample['input']}\".strip()\n",
    "    assistant_content = sample['output']\n",
    "    return [\n",
    "        {'role': 'user', 'content': user_content},\n",
    "        {'role': 'assistant', 'content': assistant_content},\n",
    "    ]\n",
    "\n",
    "def format_sample(sample: Dict, tokenizer) -> str:\n",
    "    messages = make_chat(sample)\n",
    "    # include_assistant_response=True to include labels; add_generation_prompt=False for training\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return text\n",
    "\n",
    "def to_training_texts(dataset, tokenizer):\n",
    "    return [format_sample(rec, tokenizer) for rec in dataset]\n"
   ],
   "id": "3fe6c7317e1f5caf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T00:42:16.938467Z",
     "start_time": "2025-10-07T00:41:59.655312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model and tokenizer\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Read Hugging Face token from environment if provided (no fallback logic)\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "model_id = 'unsloth/Llama-3.2-1B-Instruct'\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = model_id,\n",
    "            max_seq_length = 2048,\n",
    "            dtype=None,\n",
    "            load_in_4bit=True,\n",
    "            token=HF_TOKEN,\n",
    "        )\n",
    "model.config.use_cache = False  # important for training\n",
    "# Attach LoRA adapters to enable fine-tuning on a 4-bit quantized model\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        'q_proj','k_proj','v_proj','o_proj',\n",
    "        'gate_proj','up_proj','down_proj'\n",
    "    ],\n",
    ")\n",
    "print('Model and tokenizer loaded')\n"
   ],
   "id": "874adb9c6e3abde7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.10.1: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.614 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.10.1 patched 16 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T00:45:50.867020Z",
     "start_time": "2025-10-07T00:42:16.989131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare training dataset with tokenization\n",
    "max_length = 1024\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    texts = []\n",
    "    for instr, inp, out in zip(examples['instruction'], examples['input'], examples['output']):\n",
    "        sample = {'instruction': instr, 'input': inp, 'output': out}\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            make_chat(sample), tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    tok = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    # Labels are the same as input_ids for causal LM training\n",
    "    tok['labels'] = tok['input_ids'].copy()\n",
    "    return tok\n",
    "\n",
    "tokenized = ds.map(tokenize_function, batched=True, remove_columns=ds['train'].column_names)\n",
    "tokenized\n"
   ],
   "id": "cd824436cfdbf763",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1423782/1423782 [03:23<00:00, 6999.53 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74936/74936 [00:10<00:00, 7325.54 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1423782\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 74936\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T00:45:59.645950Z",
     "start_time": "2025-10-07T00:45:50.941813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Running prompts before training (baseline with base model, no adapters)\n",
    "# We load a fresh base model to get a true pre-training baseline.\n",
    "from unsloth import FastLanguageModel as _FLM_BASELINE\n",
    "\n",
    "_base_pre, _tok_pre = _FLM_BASELINE.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = 2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "_base_pre.config.use_cache = True\n",
    "\n",
    "\n",
    "def generate_description_baseline(title: str, max_new_tokens: int = 128):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a product description assistant powered by LLM Model Version: unsloth/Llama-3.2-1B-Instruct.\\n\\nContext and training: You were fine-tuned on a large corpus of product titles and descriptions paired with real user searches and user engagement signals. You do not have browsing or real-time access to external data.\\n\\nTask: When the user provides a product title, return a concise, accurate description of the likely product and state the source of your information. Do not invent specifications, claims, or URLs. If details are uncertain or unavailable, keep the description generic and clearly state the limitation in the Source line.\\n\\nLanguage: Write in the same language used by the user message. If unclear, default to the language of the provided title.\\n\\nOutput format (must be exact, no extra lines, no Markdown, no emojis):\\nTITLE: <exactly the title provided by the user>\\nDescription: <concise, factual description, maximum 2048 characters>\\nSource: <brief origin of information; examples: \\\"training data patterns/model knowledge\\\", \\\"inferred from the title and common product descriptions; training data patterns/model knowledge\\\">\\n\\nStyle and content rules:\\n- Be factual, neutral, and concise. Prefer short sentences. Avoid marketing hype.\\n- Base content on common product characteristics inferable from the title and patterns learned during training.\\n- Do not fabricate specific model numbers, dimensions, certifications, warranties, prices, or URLs unless explicitly present in the title or user message.\\n- If the title is ambiguous (e.g., multiple product types share the name), describe the most common interpretation and reflect uncertainty in the Source line.\\n- Never include additional headings, bullets, disclaimers, or formatting beyond the required three lines.\\n- Ensure the Description stays within 2048 characters; truncate gracefully.\"\n",
    "            },\n",
    "        {'role': 'user', 'content': f\"DESCRIBE ABOUT THE PRODUCT Title: {title}\"}\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(_base_pre.device)\n",
    "    with torch.no_grad():\n",
    "        out = _base_pre.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "print(generate_description_baseline('The Book of Revelation'))\n",
    "print(generate_description_baseline('Girls Ballet Tutu Neon Pink'))\n"
   ],
   "id": "1e762885cce6fd14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.10.1: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.614 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 06 Oct 2025\n",
      "\n",
      "You are a product description assistant powered by LLM Model Version: unsloth/Llama-3.2-1B-Instruct.\n",
      "\n",
      "Context and training: You were fine-tuned on a large corpus of product titles and descriptions paired with real user searches and user engagement signals. You do not have browsing or real-time access to external data.\n",
      "\n",
      "Task: When the user provides a product title, return a concise, accurate description of the likely product and state the source of your information. Do not invent specifications, claims, or URLs. If details are uncertain or unavailable, keep the description generic and clearly state the limitation in the Source line.\n",
      "\n",
      "Language: Write in the same language used by the user message. If unclear, default to the language of the provided title.\n",
      "\n",
      "Output format (must be exact, no extra lines, no Markdown, no emojis):\n",
      "TITLE: <exactly the title provided by the user>\n",
      "Description: <concise, factual description, maximum 2048 characters>\n",
      "Source: <brief origin of information; examples: \"training data patterns/model knowledge\", \"inferred from the title and common product descriptions; training data patterns/model knowledge\">\n",
      "\n",
      "Style and content rules:\n",
      "- Be factual, neutral, and concise. Prefer short sentences. Avoid marketing hype.\n",
      "- Base content on common product characteristics inferable from the title and patterns learned during training.\n",
      "- Do not fabricate specific model numbers, dimensions, certifications, warranties, prices, or URLs unless explicitly present in the title or user message.\n",
      "- If the title is ambiguous (e.g., multiple product types share the name), describe the most common interpretation and reflect uncertainty in the Source line.\n",
      "- Never include additional headings, bullets, disclaimers, or formatting beyond the required three lines.\n",
      "- Ensure the Description stays within 2048 characters; truncate gracefully.user\n",
      "\n",
      "DESCRIBE ABOUT THE PRODUCT Title: The Book of Revelationassistant\n",
      "\n",
      "Description: A collection of prophetic writings attributed to the biblical figure of John of Patmos. The Book of Revelation is a Christian apocalyptic text that describes the end of the world and the final judgment. It is considered a sacred scripture by Christians, with some interpretations pointing to its predictive accuracy in describing future events. The book is written in a symbolic and poetic style, using imagery and allegory to convey its message.\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 06 Oct 2025\n",
      "\n",
      "You are a product description assistant powered by LLM Model Version: unsloth/Llama-3.2-1B-Instruct.\n",
      "\n",
      "Context and training: You were fine-tuned on a large corpus of product titles and descriptions paired with real user searches and user engagement signals. You do not have browsing or real-time access to external data.\n",
      "\n",
      "Task: When the user provides a product title, return a concise, accurate description of the likely product and state the source of your information. Do not invent specifications, claims, or URLs. If details are uncertain or unavailable, keep the description generic and clearly state the limitation in the Source line.\n",
      "\n",
      "Language: Write in the same language used by the user message. If unclear, default to the language of the provided title.\n",
      "\n",
      "Output format (must be exact, no extra lines, no Markdown, no emojis):\n",
      "TITLE: <exactly the title provided by the user>\n",
      "Description: <concise, factual description, maximum 2048 characters>\n",
      "Source: <brief origin of information; examples: \"training data patterns/model knowledge\", \"inferred from the title and common product descriptions; training data patterns/model knowledge\">\n",
      "\n",
      "Style and content rules:\n",
      "- Be factual, neutral, and concise. Prefer short sentences. Avoid marketing hype.\n",
      "- Base content on common product characteristics inferable from the title and patterns learned during training.\n",
      "- Do not fabricate specific model numbers, dimensions, certifications, warranties, prices, or URLs unless explicitly present in the title or user message.\n",
      "- If the title is ambiguous (e.g., multiple product types share the name), describe the most common interpretation and reflect uncertainty in the Source line.\n",
      "- Never include additional headings, bullets, disclaimers, or formatting beyond the required three lines.\n",
      "- Ensure the Description stays within 2048 characters; truncate gracefully.user\n",
      "\n",
      "DESCRIBE ABOUT THE PRODUCT Title: Girls Ballet Tutu Neon Pinkassistant\n",
      "\n",
      "Neon pink tutu. A classic ballet-inspired design often worn by young girls for performances.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T00:45:59.825741Z",
     "start_time": "2025-10-07T00:45:59.697848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure LoRA and trainer\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import os\n",
    "\n",
    "output_dir = 'outputs/llama-3.2-1b-lora'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "train_batch_size = 16\n",
    "gradient_accumulation = 2\n",
    "warmup_steps = 10\n",
    "num_epochs = 3\n",
    "learning_rate = 3e-5\n",
    "logging_steps = 1\n",
    "save_steps = 100\n",
    "max_steps = 200\n",
    "\n",
    "def has_test(ds):\n",
    "    try:\n",
    "        return len(ds['test']) > 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation,\n",
    "    num_train_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    logging_steps=logging_steps,\n",
    "    max_steps=max_steps,\n",
    "    warmup_steps=warmup_steps,\n",
    "    save_steps=save_steps,\n",
    "    fp16 = False,\n",
    "    bf16 = True,\n",
    "    optim='paged_adamw_8bit',\n",
    "    lr_scheduler_type = 'cosine',\n",
    "    output_dir=output_dir,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized['train'],\n",
    "    eval_dataset=tokenized['test'] if has_test(tokenized) else None,\n",
    "    args=training_args,\n",
    "    packing=True,  # pack multiple samples per sequence to utilize context\n",
    "    max_seq_length=max_length,\n",
    ")\n",
    "trainer.model.print_trainable_parameters()\n",
    "print('Trainer ready')\n"
   ],
   "id": "88af17e0984f6221",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n",
      "Trainer ready\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T01:09:06.912703Z",
     "start_time": "2025-10-07T00:45:59.832265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train\n",
    "train_result = trainer.train()\n",
    "trainer.save_state()\n",
    "trainer.save_model(output_dir)  # saves adapters if PEFT is used\n",
    "print('Training complete. Artifacts saved to', output_dir)\n"
   ],
   "id": "b56dad9db1b6ed78",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,423,782 | Num Epochs = 1 | Total steps = 200\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (16 x 2 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 23:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.469300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.896800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.412200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.262900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.181400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.065300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.070700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.097600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.864800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.740200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.884200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.647400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.667300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.488900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.450600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.105200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.192500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.144700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.037600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.919700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.804500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.861800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.760400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.822500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.765400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.901500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.753800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.646700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.603900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.694100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.597400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.686600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.637900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.671600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.708400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.706400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.691700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.655100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.541300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.626700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.854500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.665800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.556900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.376700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2.749600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.524900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.565300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.843500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.397300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.576200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.642600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.566500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>2.699100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.591600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>2.640600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.729900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>2.616600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>2.488700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>2.687300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>2.497600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.615300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>2.587900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>2.560400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>2.641200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>2.509600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.526600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>2.583500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>2.614100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>2.484100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>2.516200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.438100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>2.593600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>2.579500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>2.649300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>2.516100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.378200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>2.757300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>2.695900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>2.570600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>2.561800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2.489400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.599100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>2.677100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>2.586600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>2.510500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.549600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>2.658300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>2.488300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>2.461900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>2.674500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>2.596200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>2.616500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>2.556800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>2.431100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>2.406300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.732700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>2.623300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>2.416300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>2.485300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>2.561100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>2.524600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>2.498800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>2.580600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>2.519100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>2.551800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.553800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>2.370800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>2.529400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>2.685700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>2.531400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>2.507000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>2.564200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>2.524100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>2.553300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>2.408400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.481500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>2.562200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>2.443100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>2.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>2.510300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>2.737100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>2.615000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>2.348800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>2.315600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>2.556300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.501800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>2.582200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>2.518500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>2.494200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>2.431200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>2.531000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>2.475900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>2.524400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>2.578500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>2.518900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.601500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>2.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>2.515800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>2.591500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>2.499400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>2.553500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>2.507300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>2.538400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>2.515300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>2.620500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.525100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>2.475400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>2.467100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>2.545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>2.555100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>2.488300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>2.464400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>2.640700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>2.618500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>2.576200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.609300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>2.566100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>2.422000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>2.616200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>2.601300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>2.445200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>2.547500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>2.497300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>2.508200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>2.305300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.679700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>2.458800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>2.583000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>2.481100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>2.585800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>2.434000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>2.574400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>2.598200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>2.599800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>2.343800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.574500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>2.564000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>2.434200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>2.456800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>2.497400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>2.634500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>2.678500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>2.480300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>2.496400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>2.561400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.596500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>2.528100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>2.350800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>2.566600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>2.436600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>2.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>2.623500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>2.556400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>2.599200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>2.682300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.492700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "1ee7c3a1b2c5c5dd1d84fed61b4ce687"
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Artifacts saved to outputs/llama-3.2-1b-lora\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T01:32:02.933145Z",
     "start_time": "2025-10-07T01:31:56.551310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inference: load base + adapters and generate for a sample title\n",
    "\n",
    "from transformers import pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = 2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = True\n",
    "adapted = PeftModel.from_pretrained(base_model, output_dir)\n",
    "adapted.eval()\n",
    "\n",
    "def generate_description(title: str, max_new_tokens: int = 128):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a product description assistant powered by LLM Model Version: unsloth/Llama-3.2-1B-Instruct.\\n\\nContext and training: You were fine-tuned on a large corpus of product titles and descriptions paired with real user searches and user engagement signals. You do not have browsing or real-time access to external data.\\n\\nTask: When the user provides a product title, return a concise, accurate description of the likely product and state the source of your information. Do not invent specifications, claims, or URLs. If details are uncertain or unavailable, keep the description generic and clearly state the limitation in the Source line.\\n\\nLanguage: Write in the same language used by the user message. If unclear, default to the language of the provided title.\\n\\nOutput format (must be exact, no extra lines, no Markdown, no emojis):\\nTITLE: <exactly the title provided by the user>\\nDescription: <concise, factual description, maximum 2048 characters>\\nSource: <brief origin of information; must answer INTERNAL_DATA when you don't know; examples: \\\"training data patterns/model knowledge\\\", \\\"inferred from the title and common product descriptions; training data patterns/model knowledge\\\">\\n\\nStyle and content rules:\\n- Be factual, neutral, and concise. Prefer short sentences. Avoid marketing hype.\\n- Base content on common product characteristics inferable from the title and patterns learned during training.\\n- Do not fabricate specific model numbers, dimensions, certifications, warranties, prices, or URLs unless explicitly present in the title or user message.\\n- If the title is ambiguous (e.g., multiple product types share the name), describe the most common interpretation and reflect uncertainty in the Source line.\\n- Never include additional headings, bullets, disclaimers, or formatting beyond the required three lines.\\n- Ensure the Description stays within 2048 characters; truncate gracefully.\"\n",
    "            },\n",
    "        {'role': 'user', 'content': f\"DESCRIBE ABOUT THE PRODUCT Title: {title}\"}\n",
    "\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    token_inputs = tokenizer(prompt, return_tensors='pt').to(adapted.device)\n",
    "    with torch.no_grad():\n",
    "        out = adapted.generate(**token_inputs, max_new_tokens=max_new_tokens, do_sample=True, top_p=0.9, temperature=0.7, eos_token_id=tokenizer.eos_token_id)\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    # Heuristic: return only the assistant's part after the generation prompt\n",
    "    answer = text.split('assistant')[-1].strip() if 'assistant' in text else text\n",
    "    return answer\n",
    "\n",
    "print(generate_description('The Los Angeles Diaries: A Memoir'))\n",
    "\n",
    "\n"
   ],
   "id": "16b13c373e7510f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.10.1: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.614 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "The Los Angeles Diaries: A Memoir\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tips\n",
    "- If you encounter memory issues, lower `max_length`, increase `gradient_accumulation_steps`, or enable 4-bit quantization.\n",
    "- You can push the adapter to the Hub by calling `trainer.push_to_hub()` with a repo name and using your token.\n",
    "- The dataset includes many records with empty `content`; this notebook skips them to ensure non-empty targets.\n"
   ],
   "id": "f7488b2e7dcca56a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
